{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7156195,"sourceType":"datasetVersion","datasetId":4132749}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-25T09:33:09.500103Z","iopub.execute_input":"2024-03-25T09:33:09.500465Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#subjects or patient\nsubjects=pd.read_csv(\"/kaggle/input/subjects.csv\")\n#events \nevents=pd.read_csv(\"/kaggle/input/events.csv\")\n#tasks \ntask=pd.read_csv(\"/kaggle/input/tasks.csv\")\n#defog metadata has subject's data  in home \ndefog_metadata=pd.read_csv(\"/kaggle/input/defog_metadata.csv\")\n#tdcsfog metadata has subject's data in lab \ntdcs_metadata=pd.read_csv(\"/kaggle/input/tdcsfog_metadata.csv\")\n#test data \ntest = glob.glob('/kaggle/input/test/defog/**')\n#train data\ntrain_tdcs=glob.glob('/kaggle/input/train/tdcsfog/**')\ntrain_defog=glob.glob('/kaggle/input/train/defog/**')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#function to assign Id to a dataset row from its filename\ndef readfiles(file_path):\n    df=pd.read_csv(file_path)\n    df['Id'] = file_path.split(\"/\")[-1].split(\".\")[0]\n    \n    return df\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test Data first\ntest_defog = pd.concat([readfiles(f) for f in test]).fillna(0);   \ntest_defog\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#giving id to train/defog data\ntrain_defog = pd.concat([readfiles(f) for f in train_defog]).fillna(0);   \ntrain_defog","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#subject,defog_metadata\nsubject_defog_metadata=pd.merge(subjects,defog_metadata,on=\"Subject\",how=\"inner\")\nsubject_defog_metadata","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#defog_metadata+subject+train_defog\ndefog_final=pd.merge(train_defog,subject_defog_metadata,on=\"Id\",how=\"inner\")\ndefog_final","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#function to reduce memory size\ndef reduce_memory_usage(df):\n    \n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype.name\n        if ((col_type != 'datetime64[ns]') & (col_type != 'category')):\n            if (col_type != 'object'):\n                c_min = df[col].min()\n                c_max = df[col].max()\n\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n\n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        pass\n            else:\n                df[col] = df[col].astype('category')\n    mem_usg = df.memory_usage().sum() / 1024 ** 2 \n    print(\"Memory usage became: \",mem_usg,\" MB\")\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#reducing memory size\ndefog_final=reduce_memory_usage(defog_final)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"defog_final1=pd.merge(defog_final[:10000],events,on=\"Id\",how=\"inner\")\ndefog_final1\n\n# defog_final1.groupby(\"Visit_y\").count()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"defog_final1.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"columns_to_remove = ['Walking', 'Turn', 'StartHesitation','Valid','Task','Subject','Visit_y','Id']\n\ndefog_final1 = defog_final1.drop(columns=columns_to_remove)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"defog_final1.groupby(\"Type\").count()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#binarization of gender\ndefog_final1[\"Sex\"]=defog_final1[\"Sex\"].map({\"M\":1,\"F\":0})\ndefog_final1[\"Medication\"]=defog_final1[\"Medication\"].map({\"on\":1,\"off\":0})\n\n# defog_final1[\"Type\"]=defog_final1[\"Type\"].map({\"Walking\":10,\"Turn\":11,\"StartHesitation\":12})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Changing type into binary classed i.e 0 and 1\n# defog_final1=pd.get_dummies(defog_final1, columns=['Type'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"moved_column = defog_final1.pop(\"Type\")\ndefog_final1[\"Type\"] = moved_column\ndefog_final1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#reduce defog_final1 \ndefog_final1=reduce_memory_usage(defog_final1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"defog_final1.groupby(\"Type\").count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = defog_final1.iloc[:, :-1]  # Features\ny = defog_final1.iloc[:, -1]   # Target variable\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**GAUSSIAN NAIVE BAYESIAN  WITH ROC,AuC****","metadata":{}},{"cell_type":"markdown","source":"**LATER**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import label_binarize, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport numpy as np\n\nclass DecisionTreeID3:\n    def __init__(self, max_depth=None):\n        self.max_depth = max_depth\n\n    def fit(self, X, y):\n        self.tree = self._grow_tree(X, y)\n\n    def _grow_tree(self, X, y, depth=0):\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n        # Stopping criteria\n        if self.max_depth is not None and depth == self.max_depth or n_classes == 1:\n            return {'value': np.bincount(y).argmax()}\n        # Find best split\n        best_split = self._find_best_split(X, y)\n        if best_split is None:\n            return {'value': np.bincount(y).argmax()}\n        # Split data\n        left_indices = X[:, best_split['feature']] <= best_split['threshold']\n        right_indices = ~left_indices\n        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n        return {\n            'feature': best_split['feature'],\n            'threshold': best_split['threshold'],\n            'left': left_subtree,\n            'right': right_subtree\n        }\n\n    def _find_best_split(self, X, y):\n        n_samples, n_features = X.shape\n        n_classes = len(np.unique(y))\n        best_gini = float('inf')\n        best_split = None\n\n        for feature_idx in range(n_features):\n            thresholds = np.unique(X[:, feature_idx])\n            for threshold in thresholds:\n                left_indices = X[:, feature_idx] <= threshold\n                right_indices = ~left_indices\n\n                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n                    continue\n\n                gini_left = self._gini_impurity(y[left_indices])\n                gini_right = self._gini_impurity(y[right_indices])\n\n                weighted_gini = (np.sum(left_indices) / n_samples) * gini_left + (\n                            np.sum(right_indices) / n_samples) * gini_right\n\n                if weighted_gini < best_gini:\n                    best_gini = weighted_gini\n                    best_split = {\n                        'feature': feature_idx,\n                        'threshold': threshold\n                    }\n        return best_split\n\n    def _gini_impurity(self, y):\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / len(y)\n        gini = 1 - np.sum(probabilities ** 2)\n        return gini\n\n    def predict(self, X):\n        return np.array([self._predict_tree(x, self.tree) for x in X])\n\n    def _predict_tree(self, x, tree):\n        if 'value' in tree:\n            return tree['value']\n        feature_value = x[tree['feature']]\n        if feature_value <= tree['threshold']:\n            return self._predict_tree(x, tree['left'])\n        else:\n            return self._predict_tree(x, tree['right'])\n\n    def predict_proba(self, X):\n        return np.array([self._predict_proba_tree(x, self.tree) for x in X])\n\n    def _predict_proba_tree(self, x, tree):\n        if 'value' in tree:\n            return np.bincount([tree['value']], minlength=len(np.unique(y))) / np.sum(\n                np.bincount([tree['value']], minlength=len(np.unique(y))))\n        feature_value = x[tree['feature']]\n        if feature_value <= tree['threshold']:\n            return self._predict_proba_tree(x, tree['left'])\n        else:\n            return self._predict_proba_tree(x, tree['right'])\n    \n    \n\n\n# Example usage:\nencoder = LabelEncoder()\n\ny_bin = label_binarize(encoder.fit_transform(y),classes=[0,1,2])\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n\n\n\nX_train = np.array(X_train)  # Your training data\ny_train = np.array(y_train)  # Your training labels\n\nX_test = np.array(X_test)   # Your test data\n\n\nnb_classifier = DecisionTreeID3()\nnb_classifier.fit(X_train, y_train)\n\n\n# # Make predictions\npredictions = nb_classifier.predict(X_test)\n\n\n# # Print or use predictions as needed\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**METRICS FOR THE MODEL**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Assuming you have the true labels for the test set\ny_true = y_test\n\n# Convert predicted probabilities to predicted labels (using argmax)\n# y_pred = y_score.argmax(axis=1)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true,predictions)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_true,predictions)\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\n# Print classification report\nclass_names = encoder.classes_\nclassification_rep = classification_report(y_true,predictions)\nprint('Classification Report:')\nprint(classification_rep)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# probabilities = nb_classifier.predict_proba(X_test)\n\n# # Calculate AUC for each class\n# auc_scores = []\n# for i, cls in enumerate(nb_classifier.unique_classes):\n#     class_labels = (y_test == cls).astype(int)\n#     class_probs = probabilities[:, i]\n#     auc = roc_auc_score(class_labels, class_probs)\n#     auc_scores.append(auc)\n#     print(f'AUC for Class {cls}: {auc:.2f}')\n\n# # Plot ROC curves for each class\n# plt.figure(figsize=(8, 8))\n# for i, cls in enumerate(nb_classifier.unique_classes):\n#     class_labels = (y_test == cls).astype(int)\n#     class_probs = probabilities[:, i]\n#     fpr, tpr, _ = roc_curve(class_labels, class_probs)\n#     plt.plot(fpr, tpr, label=f'Class {cls} (AUC = {auc_scores[i]:.2f})')\n\n# plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n# plt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass')\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.legend()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Naive Bayesian CLassifier","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TEST DATA\n**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_final.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # df = pd.DataFrame(test_final)\n# test_final.to_csv('Test1.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\njoblib.dump(nb_classifier, 'customm_decision_tree.pkl')\nprint(f'Download your model [here](sandbox:/kaggle/working/your_model_filename.joblib)')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open('custom_dt_model.pkl', 'wb') as model_file:\n    pickle.dump(nb_classifier, model_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model=joblib.load(\"dc.pkl\")\nmodel.predict(X_test[:10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import csv\n# classes=[\"sh\",\"turn\",\"walk\"]\n# d_classes_pred = []\n# for i in y_pred:\n#     class_index = np.argmax(i)\n#     d_class = classes[class_index]\n#     d_classes_pred.append(d_class)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}